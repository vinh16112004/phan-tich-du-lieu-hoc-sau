{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PH√ÇN T√çCH D·ªÆ LI·ªÜU D·∫†NG VƒÇN B·∫¢N V·ªöI NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] bcp47............... BCP-47 Language Tags\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#1. Gi·ªõi thi·ªáu v·ªÅ th∆∞ vi·ªán NLTK:\n",
    "import nltk\n",
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "gb = nltk.corpus.gutenberg\n",
    "print(\"Gutenberg files : \", gb.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
    "len(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'The',\n",
       " 'Tragedie',\n",
       " 'of',\n",
       " 'Macbeth',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " '1603',\n",
       " ']']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth [:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'The',\n",
       "  'Tragedie',\n",
       "  'of',\n",
       "  'Macbeth',\n",
       "  'by',\n",
       "  'William',\n",
       "  'Shakespeare',\n",
       "  '1603',\n",
       "  ']'],\n",
       " ['Actus', 'Primus', '.'],\n",
       " ['Scoena', 'Prima', '.'],\n",
       " ['Thunder', 'and', 'Lightning', '.'],\n",
       " ['Enter', 'three', 'Witches', '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#H√†m tr√™n tr√≠ch ra 10 t·ª´ ƒë·∫ßu ti√™n l√† ti√™u ƒë·ªÅ c·ªßa t·∫≠p tin, d·∫•u ngo·∫∑c vu√¥ng ƒë∆∞·ª£c t√≠nh l√† 1.Ta mu·ªën tr√≠ch 5 c√¢u ƒë·∫ßu ti√™n c·ªßa t·∫≠p tin (m·ªôt c√¢u ƒë∆∞·ª£c k·∫πp trong c·∫∑p ngo·∫∑c vu√¥ng), ta d√πng h√†msent()\n",
    "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
    "macbeth_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 3 of 3 matches:\n",
      "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
      "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
      " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
     ]
    }
   ],
   "source": [
    "#2. T√¨m 1 t·ª´ v·ªõi NLTK:\n",
    "#T√¨m t·ª´ ‚ÄòStage‚Äô xu·∫•t hi·ªán trong vƒÉn b·∫£n text\n",
    "text = nltk.Text(macbeth)\n",
    "text.concordance('Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_. bloody_: the_,\n"
     ]
    }
   ],
   "source": [
    "#T√¨m t·ª´ xu·∫•t hi·ªán tr∆∞·ªõc v√† sau t·ª´ ‚ÄòStage‚Äô\n",
    "text.common_contexts(['Stage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day time face warre ayre king bleeding man reuolt serieant like\n",
      "knowledge broyle shew head spring heeles hare thane skie\n"
     ]
    }
   ],
   "source": [
    "#T√¨m t·ª´ t∆∞∆°ng t·ª± t·ª´ ‚ÄòStage‚Äô\n",
    "text.similar('Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " ('the', 531),\n",
       " (':', 477),\n",
       " ('and', 376),\n",
       " ('I', 333),\n",
       " ('of', 315),\n",
       " ('to', 311),\n",
       " ('?', 241)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Ph√¢n t√≠ch t·∫ßn s·ªë c·ªßa c√°c t·ª´\n",
    "fd = nltk.FreqDist(macbeth)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mu·ªën download stopword\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " \"isn't\",\n",
       " 'these',\n",
       " 'such',\n",
       " 'o',\n",
       " \"needn't\",\n",
       " \"we're\",\n",
       " 'each',\n",
       " 'hadn',\n",
       " 'was']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mu·ªën xem c√°c stopword trong ti·∫øng Anh, d√πng l·ªánh\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "print(len(sw))\n",
    "list(sw)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14946"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#C√≥ 179 stopword trong t·ª´ v·ª±ng ti·∫øng Anh. Ta s·∫Ω lo·∫°i b·ªè c√°c t·ª´ stopword trong bi·∫øn macbeth\n",
    "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
    "len(macbeth_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " (':', 477),\n",
       " ('?', 241),\n",
       " ('Macb', 137),\n",
       " ('haue', 117),\n",
       " ('-', 100),\n",
       " ('Enter', 80),\n",
       " ('thou', 63)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "fd = nltk.FreqDist(macbeth_filtered)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('macb', 137),\n",
       " ('haue', 122),\n",
       " ('thou', 90),\n",
       " ('enter', 81),\n",
       " ('shall', 68),\n",
       " ('macbeth', 62),\n",
       " ('vpon', 62),\n",
       " ('thee', 61),\n",
       " ('macd', 58),\n",
       " ('vs', 57)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#B√¢y gi·ªù, 10 t·ª´ ph·ªï bi·∫øn nh·∫•t ƒë·∫ßu ti√™n ƒë∆∞·ª£c tr·∫£ v·ªÅ, c√°c stopword ƒë√£ ƒë∆∞·ª£c lo·∫°i b·ªè nh∆∞ng v·∫´n c√≤n\n",
    "#c√°c d·∫•u c√¢u, ta c·∫ßn lo·∫°i b·ªè c√°c d·∫•u c√¢u theo l·ªánh sau\n",
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
    "fd = nltk.FreqDist(macbeth_filtered2)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assassination',\n",
       " 'Chamberlaines',\n",
       " 'Distinguishes',\n",
       " 'Gallowgrosses',\n",
       " 'Metaphysicall',\n",
       " 'Northumberland',\n",
       " 'Voluptuousnesse',\n",
       " 'commendations',\n",
       " 'multitudinous',\n",
       " 'supernaturall',\n",
       " 'vnaccompanied']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. L·ª±a ch·ªçn c√°c t·ª´ trong vƒÉn b·∫£n\n",
    "long_words = [w for w in macbeth if len(w)> 12]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Auaricious',\n",
       " 'Gracious',\n",
       " 'Industrious',\n",
       " 'Iudicious',\n",
       " 'Luxurious',\n",
       " 'Malicious',\n",
       " 'Obliuious',\n",
       " 'Pious',\n",
       " 'Rebellious',\n",
       " 'compunctious',\n",
       " 'furious',\n",
       " 'gracious',\n",
       " 'pernicious',\n",
       " 'pernitious',\n",
       " 'pious',\n",
       " 'precious',\n",
       " 'rebellious',\n",
       " 'sacrilegious',\n",
       " 'serious',\n",
       " 'spacious',\n",
       " 'tedious']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#R√∫t tr√≠ch c√°c t·ª´ c√≥ ch·ª©a chu·ªói ‚Äòious‚Äô\n",
    "ious_words = [w for w in macbeth if 'ious' in w]\n",
    "ious_words = set(ious_words)\n",
    "sorted(ious_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('enter', 'macbeth'), 16),\n",
       " (('exeunt', 'scena'), 15),\n",
       " (('thane', 'cawdor'), 13),\n",
       " (('knock', 'knock'), 10),\n",
       " (('st', 'thou'), 9),\n",
       " (('thou', 'art'), 9),\n",
       " (('lord', 'macb'), 9),\n",
       " (('haue', 'done'), 8),\n",
       " (('macb', 'haue'), 8),\n",
       " (('good', 'lord'), 8),\n",
       " (('let', 'vs'), 7),\n",
       " (('enter', 'lady'), 7),\n",
       " (('wee', 'l'), 7),\n",
       " (('would', 'st'), 6),\n",
       " (('macbeth', 'macb'), 6)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Bigrams v√† collocations\n",
    "#L·ªçc c√°c bigram sau khi ƒë√£ lo·∫°i c√°c stopword v√† c√°c d·∫•u c√¢u, d√πng l·ªánh sau:\n",
    "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
    "bgrms.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('knock', 'knock', 'knock'), 6),\n",
       " (('enter', 'macbeth', 'macb'), 5),\n",
       " (('enter', 'three', 'witches'), 4),\n",
       " (('exeunt', 'scena', 'secunda'), 4),\n",
       " (('good', 'lord', 'macb'), 4),\n",
       " (('three', 'witches', '1'), 3),\n",
       " (('exeunt', 'scena', 'tertia'), 3),\n",
       " (('thunder', 'enter', 'three'), 3),\n",
       " (('exeunt', 'scena', 'quarta'), 3),\n",
       " (('scena', 'prima', 'enter'), 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ngo√†i bigram ra, c√≤n c√≥ trigram, s·ª± k·∫øt h·ª£p c·ªßa 3 t·ª´, ta d√πng l·ªánh trigrams()\n",
    "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))\n",
    "tgrms.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. S·ª≠ d·ª•ng vƒÉn b·∫£n tr√™n m·∫°ng\n",
    "#Import th∆∞ vi·ªán v√† m·ªü url ƒë·ªÉ ƒë·ªçc file\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Thay b·∫±ng c√°c l·ªánh sau:\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf-8-sig')\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*',\n",
       " '*',\n",
       " '*',\n",
       " 'START',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'EBOOK',\n",
       " '2554',\n",
       " '*',\n",
       " '*']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Th·ª±c hi·ªán c√°c l·ªánh sau:\n",
    "tokens = nltk.word_tokenize (raw)\n",
    "webtext = nltk.Text (tokens)\n",
    "webtext[:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. R√∫t tr√≠ch vƒÉn b·∫£n t·ª´ trang html\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8. Ph√¢n t√≠ch c·∫£m x√∫c ng∆∞·ªùi d√πng\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "reviews = nltk.corpus.movie_reviews\n",
    "documents = [(list(reviews.words(fileid)), category)\n",
    "for category in reviews.categories()\n",
    "for fileid in reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for more than a decade , anjelica huston has been one of america ' s finest actresses . in her directorial debut , bastard out of carolina , based upon dorothy allison ' s largely autobiographical book on child abuse and poverty in the south , huston displays impressive proficiency behind the camera as well . the film tells the story of ruth anne boatwright , nicknamed bone by one of her uncles at birth , in post - world war ii south carolina . through voice - over ( narration done by laura dern ) , we find that she nearly didn ' t survive birth , as her single mother anney , played by jennifer jason leigh , was thrown through the windshield during an automobile accident . that ' s just the beginning of her troubles . bone is deemed illegitimate on her birth certificate , a social stigma which her mother and her large poor family obsesses over for years to come . anney meets and weds lyle , a kind man and good stepfather whom gives bone a baby sister before dying in an automobile accident . anney ' s brother earle , played by michael rooker , introduces her to glen , played by ron eldard , a co - worker at the mill . glen , the black sheep of a wealthy family ( \" when are you gonna make your daddy proud ? i ' ll tell you when - never ! \" he ' s told ) and filled with bitterness , courts anney and eventually wins her , despite warnings of a violent temper . when anney miscarries with his much - prized baby boy , glen loses his capability to supress his violent outbursts and begins to direct them at bone , beating and abusing her , while the love - dependent anney tries to turn a blind eye and justify his behaviour . things turn for the worse when glen loses his job and the family of four sink into impoverishment . while set several decades ago , this story remains completely relevant in today ' s society , where abused wives return to their husbands , and abuse committed against children by one spouse is routinely overlooked by the other . the real star of this domestic drama is not top - billed jennifer jason leigh , but child actress jena malone , who plays bone in a wrenching and wholly impressive debut performance . her work is of award - calibre , and in this age in which a preponderence of superb child performers have emerged ( kirsten dunst , tina majorino , anna paquin , natalie portman , christina ricci , elijah wood ) , malone ' s performance stands up well . leigh ' s performance in bastard out of carolina is fine , although she ' s not really given much to do , and ron eldard work is very solid , skillfully manouvering his character ' s moods in and out of explosive rage . one can always sense that underneath his glen is a pressure - cooker ticking away . of the supporting cast , special note must be made of the criminally underrated michael rooker ' s performance . there ' s a bit too much generic formulaism in the screenplay for my taste , from the various characters spouting off country bumpkinisms , to the familiar sight of a dried - up uptight wise old granny ( played by grace zabriskie ) sitting on the porch rocking chair , to impassioned soliloquies better suited for a stage performance . still , it cannot be denied that anne meredith ' s screenplay contains a wallop of emotional power . huston ' s direction of the film is surehanded and impressive , giving the film good pacing and eliciting good performances from her cast , particularly the young malone . her sequences of child abuse are shot in a hard - hitting emotional , rather than graphic , manner , and are very effective . huston ' s talent with camera positioning is wonderful , from a visually striking shot done through a fan , to an extended sequence in the front seat of an automobile . the film has the toned - down feel of a television production ; bastard out of carolina was originally composed and cut for broadcast on the tnt cable station , who ended up declining the film due to sentiments that scenes of abuse and rape in the film were too disturbingly realistic for their audience . perhaps some sequences of the film may be unpleasant , but as a whole bastard out of carolina adds up to a powerful viewer experience worth seeing .\n"
     ]
    }
   ],
   "source": [
    "#Xem n·ªôi dung review ƒë·∫ßu ti√™n (d√≤ng 0, c·ªôt 0)\n",
    "first_review = ' '.join(documents[0][0])\n",
    "print(first_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ta c·∫ßn t·∫°o b·∫£ng ph√¢n ph·ªëi t·∫ßn s·ªë c√°c t·ª´ trong copus, b·∫£ng n√†y c·∫ßn chuy·ªÉn sang d·∫°ng list, ta d√πng h√†m list()\n",
    "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
    "word_features = list(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sau ƒë√≥, b∆∞·ªõc ti·∫øp theo l√† x√°c ƒë·ªãnh m·ªôt h√†m ƒë·ªÉ t√≠nh to√°n c√°c ƒë·∫∑c tr∆∞ng, t·ª©c l√† nh·ªØng t·ª´ ƒë·ªß quan\n",
    "#tr·ªçng ƒë·ªÉ thi·∫øt l·∫≠p √Ω ki·∫øn c·ªßa m·ªôt review\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Khi b·∫°n ƒë·ªãnh nghƒ©a h√†m document_features(), b·∫°n t·∫°o 1 t·∫≠p c√°c documents\n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T·∫°o t·∫≠p train v√† t·∫≠p test: 1500 d√≤ng ƒë·∫ßu d√πng cho t·∫≠p train v√† 500 d√≤ng c√≤n l·∫°i d√πng cho t·∫≠p test\n",
    "#ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh\n",
    "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n"
     ]
    }
   ],
   "source": [
    "#D√πng thu·∫≠t to√°n Na√Øve Bayes ƒë·ªÉ ph√¢n lo·∫°i, d√πng th∆∞ vi·ªán NLTK. Sau ƒë√≥ t√≠nh to√°n ƒë·ªô ch√≠nh x√°c\n",
    "#c·ªßa thu·∫≠t to√°n\n",
    "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   blame = True              neg : pos    =     13.6 : 1.0\n",
      "                   awful = True              neg : pos    =     12.8 : 1.0\n",
      "               portrayed = True              pos : neg    =     11.1 : 1.0\n",
      "                 cliches = True              neg : pos    =      9.4 : 1.0\n",
      "                     joy = True              pos : neg    =      9.2 : 1.0\n",
      "                  steals = True              pos : neg    =      9.2 : 1.0\n",
      "                   worst = True              neg : pos    =      8.8 : 1.0\n",
      "               stupidity = True              neg : pos    =      8.0 : 1.0\n",
      "                poignant = True              pos : neg    =      7.9 : 1.0\n",
      "                 creates = True              pos : neg    =      7.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Ch√∫ng ta ƒë√£ ho√†n t·∫•t vi·ªác ph√¢n t√≠ch, d∆∞·ªõi ƒë√¢y c√°c t·ª´ v·ªõi tr·ªçng s·ªë l·ªõn nh·∫•t c·ªßa c√°c review ƒë∆∞·ª£c\n",
    "#ƒë√°nh gi√° l√† positive v√† negative\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 9.B√†i t·∫≠p √°p d·ª•ng "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#1. Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ li·ªát k√™ c√°c t√™n c·ªßa copus.\n",
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danh s√°ch ng√¥n ng·ªØ c√≥ stopwords trong NLTK:\n",
      "albanian, arabic, azerbaijani, basque, belarusian, bengali, catalan, chinese, danish, dutch, english, finnish, french, german, greek, hebrew, hinglish, hungarian, indonesian, italian, kazakh, nepali, norwegian, portuguese, romanian, russian, slovene, spanish, swedish, tajik, turkish\n",
      "\n",
      "Stopwords c·ªßa english:\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n",
      "\n",
      "Stopwords c·ªßa french:\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur']\n",
      "\n",
      "Stopwords c·ªßa spanish:\n",
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo']\n",
      "\n",
      "Stopwords c·ªßa german:\n",
      "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#2. Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ li·ªát k√™ danh s√°ch c√°c stopword b·∫±ng c√°c ng√¥n ng·ªØ kh√°c nhau.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "languages = stopwords.fileids()\n",
    "print(\"Danh s√°ch ng√¥n ng·ªØ c√≥ stopwords trong NLTK:\")\n",
    "print(\", \".join(languages))\n",
    "for lang in [\"english\", \"french\", \"spanish\", \"german\", \"vietnamese\"]:\n",
    "    if lang in languages:\n",
    "        print(f\"\\nStopwords c·ªßa {lang}:\")\n",
    "        print(stopwords.words(lang)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#3. Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ ki·ªÉm tra danh s√°ch c√°c stopword b·∫±ng c√°c ng√¥n ng·ªØ kh√°c nhau\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")  # Ch·∫°y m·ªôt l·∫ßn n·∫øu ch∆∞a t·∫£i\n",
    "lang = \"english\"  # Thay b·∫±ng ng√¥n ng·ªØ b·∫°n mu·ªën ki·ªÉm tra\n",
    "word = \"the\"  # Thay b·∫±ng t·ª´ c·∫ßn ki·ªÉm tra\n",
    "print(word in stopwords.words(lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence demonstrate removing stopwords using NLTK .\n"
     ]
    }
   ],
   "source": [
    "#4. Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ lo·∫°i b·ªè c√°c stopword t·ª´ m·ªôt vƒÉn b·∫£n ƒë√£ cho.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text = \"This is an example sentence to demonstrate removing stopwords using NLTK.\"\n",
    "lang = \"english\"  # Ch·ªçn ng√¥n ng·ªØ\n",
    "\n",
    "words = word_tokenize(text)\n",
    "filtered_words = [word for word in words if word.lower() not in stopwords.words(lang)]\n",
    "\n",
    "print(\" \".join(filtered_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'don', \"isn't\", 'these', 'such', 'o', \"needn't\", \"we're\", 'each', 'hadn', 'was', 'will', 'where', 'only', \"we've\", 'against', 'after', \"i've\", 'at', 'more', 'as', 'is', 'and', 'now', 'does', 'own', 'him', 'has', \"didn't\", 'that', 'can', 't', \"hasn't\", 'if', 'll', 'she', 'while', \"i'm\", \"weren't\", 'them', 'ourselves', 'his', 'during', \"you'd\", 'off', 'me', \"that'll\", \"shan't\", 'other', \"they're\", 'ma', 've', 'in', 'it', 'through', 'have', 'whom', 'what', \"wouldn't\", 'didn', 'some', 'they', 'both', 'into', 'most', 'won', 'you', 'any', \"mightn't\", 'doesn', 'herself', 'hers', 'on', 'doing', 'we', 'those', 'the', 'because', 'mightn', 'needn', 's', 'so', \"you'll\", 'too', 'am', 'their', 'weren', 'her', 'be', \"don't\", 'he', 'a', 'until', 'your', \"she'll\", 'themselves', 'd', \"hadn't\", \"she's\", 'to', 'same', \"we'd\", 'with', 'wasn', 'aren', \"aren't\", \"shouldn't\", 'an', 'couldn', 'my', 'again', 'there', 'here', 'our', \"doesn't\", 'nor', 'down', 'for', 'how', 'ain', \"she'd\", \"i'd\", \"it'll\", 'y', 'further', 'mustn', 'of', 'before', 'isn', 'from', 'below', \"wasn't\", 'out', 'which', 'had', \"won't\", 'wouldn', \"you're\", 'few', \"they've\", 'who', 'should', \"we'll\", 'are', \"he'd\", 'shan', 'shouldn', \"they'd\", 'about', \"he'll\", \"he's\", 'very', 'theirs', 'above', 'why', 'all', \"you've\", 'under', 'myself', 'were', 'i', 'its', 'did', \"they'll\", 'been', 'by', \"it'd\", \"i'll\", \"couldn't\", 'itself', 'over', 'haven', \"it's\", \"haven't\", 'once', 'up', 'do', 'just', 're', 'being', 'having', 'or', 'then', \"mustn't\", 'himself', 'yourself', \"should've\", 'yourselves', 'hasn', 'this', 'than', 'when', 'yours', 'm', 'ours', 'between'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#5. Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK b·ªè qua c√°c stopword t·ª´ danh s√°ch c√°c stopword.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "lang = \"english\"\n",
    "default_stopwords = set(stopwords.words(lang))\n",
    "\n",
    "# Danh s√°ch stopwords c·∫ßn lo·∫°i b·ªè\n",
    "custom_exclude = {\"not\", \"but\", \"no\"}\n",
    "\n",
    "# C·∫≠p nh·∫≠t danh s√°ch stopwords m·ªõi\n",
    "filtered_stopwords = default_stopwords - custom_exclude\n",
    "\n",
    "print(filtered_stopwords)  # In danh s√°ch stopwords sau khi lo·∫°i b·ªè\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ ƒê·ªãnh nghƒ©a: a machine for performing calculations automatically\n",
      "üî∏ V√≠ d·ª•: []\n",
      "\n",
      "üîπ ƒê·ªãnh nghƒ©a: an expert at calculation (or at operating calculating machines)\n",
      "üî∏ V√≠ d·ª•: []\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#6Vi·∫øt m·ªôt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ t√¨m ƒë·ªãnh nghƒ©a v√† v√≠ d·ª• c·ªßa m·ªôt t·ª´ ƒë√£ cho b·∫±ng WordNet t·ª´ Wikipedia,\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "word = \"computer\"  # Nh·∫≠p t·ª´ c·∫ßn tra c·ª©u\n",
    "\n",
    "synsets = wordnet.synsets(word)\n",
    "\n",
    "if synsets:\n",
    "    for syn in synsets[:3]:  # L·∫•y 3 k·∫øt qu·∫£ ƒë·∫ßu ti√™n\n",
    "        print(f\"üîπ ƒê·ªãnh nghƒ©a: {syn.definition()}\")\n",
    "        print(f\"üî∏ V√≠ d·ª•: {syn.examples()}\\n\")\n",
    "else:\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y t·ª´ n√†y trong WordNet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ T·ª´ ƒë·ªìng nghƒ©a c·ªßa 'happy': {'glad', 'felicitous', 'well-chosen', 'happy'}\n",
      "üî∏ T·ª´ tr√°i nghƒ©a c·ªßa 'happy': {'unhappy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#7. Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ t√¨m t·∫≠p h·ª£p c√°c t·ª´ ƒë·ªìng nghƒ©a v√† tr√°i nghƒ©a c·ªßa m·ªôt t·ª´ n√†o ƒë√≥.\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "word = \"happy\"  # Nh·∫≠p t·ª´ c·∫ßn tra c·ª©u\n",
    "\n",
    "synonyms = set()\n",
    "antonyms = set()\n",
    "\n",
    "for syn in wordnet.synsets(word):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.add(lemma.name())  # Th√™m t·ª´ ƒë·ªìng nghƒ©a\n",
    "        if lemma.antonyms():  # Ki·ªÉm tra c√≥ t·ª´ tr√°i nghƒ©a kh√¥ng\n",
    "            antonyms.add(lemma.antonyms()[0].name())\n",
    "\n",
    "print(f\"üîπ T·ª´ ƒë·ªìng nghƒ©a c·ªßa '{word}': {synonyms}\")\n",
    "print(f\"üî∏ T·ª´ tr√°i nghƒ©a c·ªßa '{word}': {antonyms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç C√°c tag li√™n quan ƒë·∫øn 'VB.*':\n",
      "VBG\n",
      "VB\n",
      "VBP\n",
      "VBN\n",
      "VBZ\n",
      "VBD\n"
     ]
    }
   ],
   "source": [
    "#8. Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ c√≥ c√°i nh√¨n t·ªïng quan v·ªÅ b·ªô tag, chi ti·∫øt c·ªßa\n",
    "#m·ªôt tag c·ª• th·ªÉ trong b·ªô tag v√† chi ti·∫øt v·ªÅ m·ªôt s·ªë b·ªô tag li√™n quan, s·ª≠ d·ª•ng bi·ªÉu th·ª©c ch√≠nh quy\n",
    "#nltk.download(\"tagsets\")\n",
    "import re\n",
    "\n",
    "pattern = \"VB.*\"  # T√¨m c√°c tag li√™n quan ƒë·∫øn ƒë·ªông t·ª´ (Verb)\n",
    "print(f\"\\nüîç C√°c tag li√™n quan ƒë·∫øn '{pattern}':\")\n",
    "\n",
    "for tag in nltk.data.load(\"help/tagsets/upenn_tagset.pickle\").keys():\n",
    "    if re.match(pattern, tag):\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ ƒê·ªô gi·ªëng nhau gi·ªØa 'car' v√† 'bus': 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#9 . Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ so s√°nh s·ª± gi·ªëng nhau c·ªßa hai danh t·ª´ ƒë√£cho.\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "word1 = \"car\"  # Nh·∫≠p danh t·ª´ 1\n",
    "word2 = \"bus\"  # Nh·∫≠p danh t·ª´ 2\n",
    "\n",
    "syn1 = wordnet.synsets(word1, pos=wordnet.NOUN)\n",
    "syn2 = wordnet.synsets(word2, pos=wordnet.NOUN)\n",
    "\n",
    "if syn1 and syn2:\n",
    "    similarity = syn1[0].wup_similarity(syn2[0])  # T√≠nh ƒë·ªô gi·ªëng nhau\n",
    "    print(f\"üîπ ƒê·ªô gi·ªëng nhau gi·ªØa '{word1}' v√† '{word2}': {similarity:.2f}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y t·ª´ trong WordNet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ ƒê·ªô gi·ªëng nhau gi·ªØa 'run' v√† 'walk': 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#10 Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ so s√°nh s·ª± gi·ªëng nhau c·ªßa hai ƒë·ªông t·ª´ ƒë√£ cho.\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "word1 = \"run\"  # Nh·∫≠p ƒë·ªông t·ª´ 1\n",
    "word2 = \"walk\"  # Nh·∫≠p ƒë·ªông t·ª´ 2\n",
    "\n",
    "syn1 = wordnet.synsets(word1, pos=wordnet.VERB)\n",
    "syn2 = wordnet.synsets(word2, pos=wordnet.VERB)\n",
    "\n",
    "if syn1 and syn2:\n",
    "    similarity = syn1[0].wup_similarity(syn2[0])  # T√≠nh ƒë·ªô gi·ªëng nhau\n",
    "    print(f\"üîπ ƒê·ªô gi·ªëng nhau gi·ªØa '{word1}' v√† '{word2}': {similarity:.2f}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y t·ª´ trong WordNet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ S·ªë l∆∞·ª£ng t√™n nam: 2943\n",
      "üî∏ S·ªë l∆∞·ª£ng t√™n n·ªØ: 5001\n",
      "\n",
      "üìå 10 t√™n nam ƒë·∫ßu ti√™n: ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim']\n",
      "üìå 10 t√™n n·ªØ ƒë·∫ßu ti√™n: ['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\names.zip.\n"
     ]
    }
   ],
   "source": [
    "#11  Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ t√¨m s·ªë l∆∞·ª£ng t√™n nam v√† n·ªØ trong c√°c t√™n\n",
    "#kho ng·ªØ li·ªáu. In t√™n 10 nam v√† n·ªØ ƒë·∫ßu ti√™n. L∆∞u √Ω: Kho vƒÉn b·∫£n t√™n ch·ª©a t·ªïng c·ªông kho·∫£ng\n",
    "#2943 nam (male.txt) v√† 5001 n·ªØ (Female.txt) t√™n. Kho ƒë∆∞·ª£c bi√™n so·∫°n b·ªüi Kantrowitz, Ross.\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "\n",
    "nltk.download(\"names\")\n",
    "\n",
    "male_names = names.words(\"male.txt\")\n",
    "female_names = names.words(\"female.txt\")\n",
    "\n",
    "print(f\"üîπ S·ªë l∆∞·ª£ng t√™n nam: {len(male_names)}\")\n",
    "print(f\"üî∏ S·ªë l∆∞·ª£ng t√™n n·ªØ: {len(female_names)}\")\n",
    "\n",
    "print(\"\\nüìå 10 t√™n nam ƒë·∫ßu ti√™n:\", male_names[:10])\n",
    "print(\"üìå 10 t√™n n·ªØ ƒë·∫ßu ti√™n:\", female_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå 15 t√™n ng·∫´u nhi√™n v·ªõi nh√£n gi·ªõi t√≠nh:\n",
      "Zach ‚Üí male\n",
      "Lari ‚Üí female\n",
      "Ehud ‚Üí male\n",
      "Hamil ‚Üí male\n",
      "Adorne ‚Üí female\n",
      "Bartolemo ‚Üí male\n",
      "Zorana ‚Üí female\n",
      "Joice ‚Üí female\n",
      "Dane ‚Üí male\n",
      "Paulo ‚Üí male\n",
      "Catherin ‚Üí female\n",
      "Nicolette ‚Üí female\n",
      "Townsend ‚Üí male\n",
      "Jens ‚Üí male\n",
      "Manya ‚Üí female\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#12.Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ in 15 k·∫øt h·ª£p ng·∫´u nhi√™n ƒë·∫ßu ti√™n ƒë∆∞·ª£c g·∫Øn\n",
    "#nh√£n nam v√† ƒë∆∞·ª£c g·∫Øn nh√£n t√™n n·ªØ t·ª´ kho t√™n.\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "\n",
    "nltk.download(\"names\")\n",
    "\n",
    "male_names = [(name, \"male\") for name in names.words(\"male.txt\")]\n",
    "female_names = [(name, \"female\") for name in names.words(\"female.txt\")]\n",
    "\n",
    "# G·ªôp danh s√°ch v√† x√°o tr·ªôn ng·∫´u nhi√™n\n",
    "combined_names = male_names + female_names\n",
    "random.shuffle(combined_names)\n",
    "\n",
    "# In 15 c·∫∑p ƒë·∫ßu ti√™n\n",
    "print(\"üìå 15 t√™n ng·∫´u nhi√™n v·ªõi nh√£n gi·ªõi t√≠nh:\")\n",
    "for name, gender in combined_names[:15]:\n",
    "    print(f\"{name} ‚Üí {gender}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå 15 k√Ω t·ª± cu·ªëi c√πng c·ªßa t√™n v·ªõi nh√£n:\n",
      "'r' ‚Üí male\n",
      "'n' ‚Üí male\n",
      "'y' ‚Üí male\n",
      "'e' ‚Üí male\n",
      "'t' ‚Üí male\n",
      "'t' ‚Üí male\n",
      "'y' ‚Üí male\n",
      "'l' ‚Üí male\n",
      "'l' ‚Üí male\n",
      "'m' ‚Üí male\n",
      "'h' ‚Üí male\n",
      "'e' ‚Üí male\n",
      "'l' ‚Üí male\n",
      "'d' ‚Üí male\n",
      "'r' ‚Üí male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#13 Vi·∫øt ch∆∞∆°ng tr√¨nh Python v·ªõi th∆∞ vi·ªán NLTK ƒë·ªÉ tr√≠ch xu·∫•t k√Ω t·ª± cu·ªëi c√πng c·ªßa t·∫•t c·∫£ c√°c t√™n\n",
    "#ƒë∆∞·ª£c g·∫Øn nh√£n v√† t·∫°o m·∫£ng m·ªõi v·ªõi ch·ªØ c√°i cu·ªëi c√πng c·ªßa m·ªói t√™n v√† nh√£n ƒë∆∞·ª£c li√™n k·∫øt.\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "\n",
    "nltk.download(\"names\")\n",
    "\n",
    "# L·∫•y danh s√°ch t√™n nam v√† n·ªØ c√πng v·ªõi nh√£n\n",
    "male_names = [(name[-1], \"male\") for name in names.words(\"male.txt\")]\n",
    "female_names = [(name[-1], \"female\") for name in names.words(\"female.txt\")]\n",
    "\n",
    "# G·ªôp danh s√°ch\n",
    "final_letters = male_names + female_names\n",
    "\n",
    "# In 15 k√Ω t·ª± cu·ªëi c√πng ƒë·∫ßu ti√™n c√πng nh√£n\n",
    "print(\"üìå 15 k√Ω t·ª± cu·ªëi c√πng c·ªßa t√™n v·ªõi nh√£n:\")\n",
    "for letter, gender in final_letters[:15]:\n",
    "    print(f\"'{letter}' ‚Üí {gender}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
