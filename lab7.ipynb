{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PHÂN TÍCH DỮ LIỆU DẠNG VĂN BẢN VỚI NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] bcp47............... BCP-47 Language Tags\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#1. Giới thiệu về thư viện NLTK:\n",
    "import nltk\n",
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "gb = nltk.corpus.gutenberg\n",
    "print(\"Gutenberg files : \", gb.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
    "len(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'The',\n",
       " 'Tragedie',\n",
       " 'of',\n",
       " 'Macbeth',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " '1603',\n",
       " ']']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth [:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'The',\n",
       "  'Tragedie',\n",
       "  'of',\n",
       "  'Macbeth',\n",
       "  'by',\n",
       "  'William',\n",
       "  'Shakespeare',\n",
       "  '1603',\n",
       "  ']'],\n",
       " ['Actus', 'Primus', '.'],\n",
       " ['Scoena', 'Prima', '.'],\n",
       " ['Thunder', 'and', 'Lightning', '.'],\n",
       " ['Enter', 'three', 'Witches', '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hàm trên trích ra 10 từ đầu tiên là tiêu đề của tập tin, dấu ngoặc vuông được tính là 1.Ta muốn trích 5 câu đầu tiên của tập tin (một câu được kẹp trong cặp ngoặc vuông), ta dùng hàmsent()\n",
    "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
    "macbeth_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 3 of 3 matches:\n",
      "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
      "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
      " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
     ]
    }
   ],
   "source": [
    "#2. Tìm 1 từ với NLTK:\n",
    "#Tìm từ ‘Stage’ xuất hiện trong văn bản text\n",
    "text = nltk.Text(macbeth)\n",
    "text.concordance('Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_. bloody_: the_,\n"
     ]
    }
   ],
   "source": [
    "#Tìm từ xuất hiện trước và sau từ ‘Stage’\n",
    "text.common_contexts(['Stage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day time face warre ayre king bleeding man reuolt serieant like\n",
      "knowledge broyle shew head spring heeles hare thane skie\n"
     ]
    }
   ],
   "source": [
    "#Tìm từ tương tự từ ‘Stage’\n",
    "text.similar('Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " ('the', 531),\n",
       " (':', 477),\n",
       " ('and', 376),\n",
       " ('I', 333),\n",
       " ('of', 315),\n",
       " ('to', 311),\n",
       " ('?', 241)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Phân tích tần số của các từ\n",
    "fd = nltk.FreqDist(macbeth)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Muốn download stopword\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " \"isn't\",\n",
       " 'these',\n",
       " 'such',\n",
       " 'o',\n",
       " \"needn't\",\n",
       " \"we're\",\n",
       " 'each',\n",
       " 'hadn',\n",
       " 'was']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Muốn xem các stopword trong tiếng Anh, dùng lệnh\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "print(len(sw))\n",
    "list(sw)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14946"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Có 179 stopword trong từ vựng tiếng Anh. Ta sẽ loại bỏ các từ stopword trong biến macbeth\n",
    "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
    "len(macbeth_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1962),\n",
       " ('.', 1235),\n",
       " (\"'\", 637),\n",
       " (':', 477),\n",
       " ('?', 241),\n",
       " ('Macb', 137),\n",
       " ('haue', 117),\n",
       " ('-', 100),\n",
       " ('Enter', 80),\n",
       " ('thou', 63)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "fd = nltk.FreqDist(macbeth_filtered)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('macb', 137),\n",
       " ('haue', 122),\n",
       " ('thou', 90),\n",
       " ('enter', 81),\n",
       " ('shall', 68),\n",
       " ('macbeth', 62),\n",
       " ('vpon', 62),\n",
       " ('thee', 61),\n",
       " ('macd', 58),\n",
       " ('vs', 57)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bây giờ, 10 từ phổ biến nhất đầu tiên được trả về, các stopword đã được loại bỏ nhưng vẫn còn\n",
    "#các dấu câu, ta cần loại bỏ các dấu câu theo lệnh sau\n",
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
    "fd = nltk.FreqDist(macbeth_filtered2)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assassination',\n",
       " 'Chamberlaines',\n",
       " 'Distinguishes',\n",
       " 'Gallowgrosses',\n",
       " 'Metaphysicall',\n",
       " 'Northumberland',\n",
       " 'Voluptuousnesse',\n",
       " 'commendations',\n",
       " 'multitudinous',\n",
       " 'supernaturall',\n",
       " 'vnaccompanied']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Lựa chọn các từ trong văn bản\n",
    "long_words = [w for w in macbeth if len(w)> 12]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Auaricious',\n",
       " 'Gracious',\n",
       " 'Industrious',\n",
       " 'Iudicious',\n",
       " 'Luxurious',\n",
       " 'Malicious',\n",
       " 'Obliuious',\n",
       " 'Pious',\n",
       " 'Rebellious',\n",
       " 'compunctious',\n",
       " 'furious',\n",
       " 'gracious',\n",
       " 'pernicious',\n",
       " 'pernitious',\n",
       " 'pious',\n",
       " 'precious',\n",
       " 'rebellious',\n",
       " 'sacrilegious',\n",
       " 'serious',\n",
       " 'spacious',\n",
       " 'tedious']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rút trích các từ có chứa chuỗi ‘ious’\n",
    "ious_words = [w for w in macbeth if 'ious' in w]\n",
    "ious_words = set(ious_words)\n",
    "sorted(ious_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('enter', 'macbeth'), 16),\n",
       " (('exeunt', 'scena'), 15),\n",
       " (('thane', 'cawdor'), 13),\n",
       " (('knock', 'knock'), 10),\n",
       " (('st', 'thou'), 9),\n",
       " (('thou', 'art'), 9),\n",
       " (('lord', 'macb'), 9),\n",
       " (('haue', 'done'), 8),\n",
       " (('macb', 'haue'), 8),\n",
       " (('good', 'lord'), 8),\n",
       " (('let', 'vs'), 7),\n",
       " (('enter', 'lady'), 7),\n",
       " (('wee', 'l'), 7),\n",
       " (('would', 'st'), 6),\n",
       " (('macbeth', 'macb'), 6)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Bigrams và collocations\n",
    "#Lọc các bigram sau khi đã loại các stopword và các dấu câu, dùng lệnh sau:\n",
    "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
    "bgrms.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('knock', 'knock', 'knock'), 6),\n",
       " (('enter', 'macbeth', 'macb'), 5),\n",
       " (('enter', 'three', 'witches'), 4),\n",
       " (('exeunt', 'scena', 'secunda'), 4),\n",
       " (('good', 'lord', 'macb'), 4),\n",
       " (('three', 'witches', '1'), 3),\n",
       " (('exeunt', 'scena', 'tertia'), 3),\n",
       " (('thunder', 'enter', 'three'), 3),\n",
       " (('exeunt', 'scena', 'quarta'), 3),\n",
       " (('scena', 'prima', 'enter'), 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ngoài bigram ra, còn có trigram, sự kết hợp của 3 từ, ta dùng lệnh trigrams()\n",
    "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))\n",
    "tgrms.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. Sử dụng văn bản trên mạng\n",
    "#Import thư viện và mở url để đọc file\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Thay bằng các lệnh sau:\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf-8-sig')\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*',\n",
       " '*',\n",
       " '*',\n",
       " 'START',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'EBOOK',\n",
       " '2554',\n",
       " '*',\n",
       " '*']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Thực hiện các lệnh sau:\n",
    "tokens = nltk.word_tokenize (raw)\n",
    "webtext = nltk.Text (tokens)\n",
    "webtext[:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. Rút trích văn bản từ trang html\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8. Phân tích cảm xúc người dùng\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "reviews = nltk.corpus.movie_reviews\n",
    "documents = [(list(reviews.words(fileid)), category)\n",
    "for category in reviews.categories()\n",
    "for fileid in reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for more than a decade , anjelica huston has been one of america ' s finest actresses . in her directorial debut , bastard out of carolina , based upon dorothy allison ' s largely autobiographical book on child abuse and poverty in the south , huston displays impressive proficiency behind the camera as well . the film tells the story of ruth anne boatwright , nicknamed bone by one of her uncles at birth , in post - world war ii south carolina . through voice - over ( narration done by laura dern ) , we find that she nearly didn ' t survive birth , as her single mother anney , played by jennifer jason leigh , was thrown through the windshield during an automobile accident . that ' s just the beginning of her troubles . bone is deemed illegitimate on her birth certificate , a social stigma which her mother and her large poor family obsesses over for years to come . anney meets and weds lyle , a kind man and good stepfather whom gives bone a baby sister before dying in an automobile accident . anney ' s brother earle , played by michael rooker , introduces her to glen , played by ron eldard , a co - worker at the mill . glen , the black sheep of a wealthy family ( \" when are you gonna make your daddy proud ? i ' ll tell you when - never ! \" he ' s told ) and filled with bitterness , courts anney and eventually wins her , despite warnings of a violent temper . when anney miscarries with his much - prized baby boy , glen loses his capability to supress his violent outbursts and begins to direct them at bone , beating and abusing her , while the love - dependent anney tries to turn a blind eye and justify his behaviour . things turn for the worse when glen loses his job and the family of four sink into impoverishment . while set several decades ago , this story remains completely relevant in today ' s society , where abused wives return to their husbands , and abuse committed against children by one spouse is routinely overlooked by the other . the real star of this domestic drama is not top - billed jennifer jason leigh , but child actress jena malone , who plays bone in a wrenching and wholly impressive debut performance . her work is of award - calibre , and in this age in which a preponderence of superb child performers have emerged ( kirsten dunst , tina majorino , anna paquin , natalie portman , christina ricci , elijah wood ) , malone ' s performance stands up well . leigh ' s performance in bastard out of carolina is fine , although she ' s not really given much to do , and ron eldard work is very solid , skillfully manouvering his character ' s moods in and out of explosive rage . one can always sense that underneath his glen is a pressure - cooker ticking away . of the supporting cast , special note must be made of the criminally underrated michael rooker ' s performance . there ' s a bit too much generic formulaism in the screenplay for my taste , from the various characters spouting off country bumpkinisms , to the familiar sight of a dried - up uptight wise old granny ( played by grace zabriskie ) sitting on the porch rocking chair , to impassioned soliloquies better suited for a stage performance . still , it cannot be denied that anne meredith ' s screenplay contains a wallop of emotional power . huston ' s direction of the film is surehanded and impressive , giving the film good pacing and eliciting good performances from her cast , particularly the young malone . her sequences of child abuse are shot in a hard - hitting emotional , rather than graphic , manner , and are very effective . huston ' s talent with camera positioning is wonderful , from a visually striking shot done through a fan , to an extended sequence in the front seat of an automobile . the film has the toned - down feel of a television production ; bastard out of carolina was originally composed and cut for broadcast on the tnt cable station , who ended up declining the film due to sentiments that scenes of abuse and rape in the film were too disturbingly realistic for their audience . perhaps some sequences of the film may be unpleasant , but as a whole bastard out of carolina adds up to a powerful viewer experience worth seeing .\n"
     ]
    }
   ],
   "source": [
    "#Xem nội dung review đầu tiên (dòng 0, cột 0)\n",
    "first_review = ' '.join(documents[0][0])\n",
    "print(first_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ta cần tạo bảng phân phối tần số các từ trong copus, bảng này cần chuyển sang dạng list, ta dùng hàm list()\n",
    "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
    "word_features = list(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sau đó, bước tiếp theo là xác định một hàm để tính toán các đặc trưng, tức là những từ đủ quan\n",
    "#trọng để thiết lập ý kiến của một review\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Khi bạn định nghĩa hàm document_features(), bạn tạo 1 tập các documents\n",
    "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo tập train và tập test: 1500 dòng đầu dùng cho tập train và 500 dòng còn lại dùng cho tập test\n",
    "#để đánh giá độ chính xác của mô hình\n",
    "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n"
     ]
    }
   ],
   "source": [
    "#Dùng thuật toán Naïve Bayes để phân loại, dùng thư viện NLTK. Sau đó tính toán độ chính xác\n",
    "#của thuật toán\n",
    "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   blame = True              neg : pos    =     13.6 : 1.0\n",
      "                   awful = True              neg : pos    =     12.8 : 1.0\n",
      "               portrayed = True              pos : neg    =     11.1 : 1.0\n",
      "                 cliches = True              neg : pos    =      9.4 : 1.0\n",
      "                     joy = True              pos : neg    =      9.2 : 1.0\n",
      "                  steals = True              pos : neg    =      9.2 : 1.0\n",
      "                   worst = True              neg : pos    =      8.8 : 1.0\n",
      "               stupidity = True              neg : pos    =      8.0 : 1.0\n",
      "                poignant = True              pos : neg    =      7.9 : 1.0\n",
      "                 creates = True              pos : neg    =      7.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Chúng ta đã hoàn tất việc phân tích, dưới đây các từ với trọng số lớn nhất của các review được\n",
    "#đánh giá là positive và negative\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 9.Bài tập áp dụng "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#1. Viết chương trình Python với thư viện NLTK để liệt kê các tên của copus.\n",
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danh sách ngôn ngữ có stopwords trong NLTK:\n",
      "albanian, arabic, azerbaijani, basque, belarusian, bengali, catalan, chinese, danish, dutch, english, finnish, french, german, greek, hebrew, hinglish, hungarian, indonesian, italian, kazakh, nepali, norwegian, portuguese, romanian, russian, slovene, spanish, swedish, tajik, turkish\n",
      "\n",
      "Stopwords của english:\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n",
      "\n",
      "Stopwords của french:\n",
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur']\n",
      "\n",
      "Stopwords của spanish:\n",
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo']\n",
      "\n",
      "Stopwords của german:\n",
      "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#2. Viết chương trình Python với thư viện NLTK để liệt kê danh sách các stopword bằng các ngôn ngữ khác nhau.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "languages = stopwords.fileids()\n",
    "print(\"Danh sách ngôn ngữ có stopwords trong NLTK:\")\n",
    "print(\", \".join(languages))\n",
    "for lang in [\"english\", \"french\", \"spanish\", \"german\", \"vietnamese\"]:\n",
    "    if lang in languages:\n",
    "        print(f\"\\nStopwords của {lang}:\")\n",
    "        print(stopwords.words(lang)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#3. Viết chương trình Python với thư viện NLTK để kiểm tra danh sách các stopword bằng các ngôn ngữ khác nhau\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")  # Chạy một lần nếu chưa tải\n",
    "lang = \"english\"  # Thay bằng ngôn ngữ bạn muốn kiểm tra\n",
    "word = \"the\"  # Thay bằng từ cần kiểm tra\n",
    "print(word in stopwords.words(lang))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence demonstrate removing stopwords using NLTK .\n"
     ]
    }
   ],
   "source": [
    "#4. Viết chương trình Python với thư viện NLTK để loại bỏ các stopword từ một văn bản đã cho.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text = \"This is an example sentence to demonstrate removing stopwords using NLTK.\"\n",
    "lang = \"english\"  # Chọn ngôn ngữ\n",
    "\n",
    "words = word_tokenize(text)\n",
    "filtered_words = [word for word in words if word.lower() not in stopwords.words(lang)]\n",
    "\n",
    "print(\" \".join(filtered_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'don', \"isn't\", 'these', 'such', 'o', \"needn't\", \"we're\", 'each', 'hadn', 'was', 'will', 'where', 'only', \"we've\", 'against', 'after', \"i've\", 'at', 'more', 'as', 'is', 'and', 'now', 'does', 'own', 'him', 'has', \"didn't\", 'that', 'can', 't', \"hasn't\", 'if', 'll', 'she', 'while', \"i'm\", \"weren't\", 'them', 'ourselves', 'his', 'during', \"you'd\", 'off', 'me', \"that'll\", \"shan't\", 'other', \"they're\", 'ma', 've', 'in', 'it', 'through', 'have', 'whom', 'what', \"wouldn't\", 'didn', 'some', 'they', 'both', 'into', 'most', 'won', 'you', 'any', \"mightn't\", 'doesn', 'herself', 'hers', 'on', 'doing', 'we', 'those', 'the', 'because', 'mightn', 'needn', 's', 'so', \"you'll\", 'too', 'am', 'their', 'weren', 'her', 'be', \"don't\", 'he', 'a', 'until', 'your', \"she'll\", 'themselves', 'd', \"hadn't\", \"she's\", 'to', 'same', \"we'd\", 'with', 'wasn', 'aren', \"aren't\", \"shouldn't\", 'an', 'couldn', 'my', 'again', 'there', 'here', 'our', \"doesn't\", 'nor', 'down', 'for', 'how', 'ain', \"she'd\", \"i'd\", \"it'll\", 'y', 'further', 'mustn', 'of', 'before', 'isn', 'from', 'below', \"wasn't\", 'out', 'which', 'had', \"won't\", 'wouldn', \"you're\", 'few', \"they've\", 'who', 'should', \"we'll\", 'are', \"he'd\", 'shan', 'shouldn', \"they'd\", 'about', \"he'll\", \"he's\", 'very', 'theirs', 'above', 'why', 'all', \"you've\", 'under', 'myself', 'were', 'i', 'its', 'did', \"they'll\", 'been', 'by', \"it'd\", \"i'll\", \"couldn't\", 'itself', 'over', 'haven', \"it's\", \"haven't\", 'once', 'up', 'do', 'just', 're', 'being', 'having', 'or', 'then', \"mustn't\", 'himself', 'yourself', \"should've\", 'yourselves', 'hasn', 'this', 'than', 'when', 'yours', 'm', 'ours', 'between'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#5. Viết chương trình Python với thư viện NLTK bỏ qua các stopword từ danh sách các stopword.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "lang = \"english\"\n",
    "default_stopwords = set(stopwords.words(lang))\n",
    "\n",
    "# Danh sách stopwords cần loại bỏ\n",
    "custom_exclude = {\"not\", \"but\", \"no\"}\n",
    "\n",
    "# Cập nhật danh sách stopwords mới\n",
    "filtered_stopwords = default_stopwords - custom_exclude\n",
    "\n",
    "print(filtered_stopwords)  # In danh sách stopwords sau khi loại bỏ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Định nghĩa: a machine for performing calculations automatically\n",
      "🔸 Ví dụ: []\n",
      "\n",
      "🔹 Định nghĩa: an expert at calculation (or at operating calculating machines)\n",
      "🔸 Ví dụ: []\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#6Viết một chương trình Python với thư viện NLTK để tìm định nghĩa và ví dụ của một từ đã cho bằng WordNet từ Wikipedia,\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "word = \"computer\"  # Nhập từ cần tra cứu\n",
    "\n",
    "synsets = wordnet.synsets(word)\n",
    "\n",
    "if synsets:\n",
    "    for syn in synsets[:3]:  # Lấy 3 kết quả đầu tiên\n",
    "        print(f\"🔹 Định nghĩa: {syn.definition()}\")\n",
    "        print(f\"🔸 Ví dụ: {syn.examples()}\\n\")\n",
    "else:\n",
    "    print(\"Không tìm thấy từ này trong WordNet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Từ đồng nghĩa của 'happy': {'glad', 'felicitous', 'well-chosen', 'happy'}\n",
      "🔸 Từ trái nghĩa của 'happy': {'unhappy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#7. Viết chương trình Python với thư viện NLTK để tìm tập hợp các từ đồng nghĩa và trái nghĩa của một từ nào đó.\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "word = \"happy\"  # Nhập từ cần tra cứu\n",
    "\n",
    "synonyms = set()\n",
    "antonyms = set()\n",
    "\n",
    "for syn in wordnet.synsets(word):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.add(lemma.name())  # Thêm từ đồng nghĩa\n",
    "        if lemma.antonyms():  # Kiểm tra có từ trái nghĩa không\n",
    "            antonyms.add(lemma.antonyms()[0].name())\n",
    "\n",
    "print(f\"🔹 Từ đồng nghĩa của '{word}': {synonyms}\")\n",
    "print(f\"🔸 Từ trái nghĩa của '{word}': {antonyms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Các tag liên quan đến 'VB.*':\n",
      "VBG\n",
      "VB\n",
      "VBP\n",
      "VBN\n",
      "VBZ\n",
      "VBD\n"
     ]
    }
   ],
   "source": [
    "#8. Viết chương trình Python với thư viện NLTK để có cái nhìn tổng quan về bộ tag, chi tiết của\n",
    "#một tag cụ thể trong bộ tag và chi tiết về một số bộ tag liên quan, sử dụng biểu thức chính quy\n",
    "#nltk.download(\"tagsets\")\n",
    "import re\n",
    "\n",
    "pattern = \"VB.*\"  # Tìm các tag liên quan đến động từ (Verb)\n",
    "print(f\"\\n🔍 Các tag liên quan đến '{pattern}':\")\n",
    "\n",
    "for tag in nltk.data.load(\"help/tagsets/upenn_tagset.pickle\").keys():\n",
    "    if re.match(pattern, tag):\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Độ giống nhau giữa 'car' và 'bus': 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#9 . Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai danh từ đãcho.\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "word1 = \"car\"  # Nhập danh từ 1\n",
    "word2 = \"bus\"  # Nhập danh từ 2\n",
    "\n",
    "syn1 = wordnet.synsets(word1, pos=wordnet.NOUN)\n",
    "syn2 = wordnet.synsets(word2, pos=wordnet.NOUN)\n",
    "\n",
    "if syn1 and syn2:\n",
    "    similarity = syn1[0].wup_similarity(syn2[0])  # Tính độ giống nhau\n",
    "    print(f\"🔹 Độ giống nhau giữa '{word1}' và '{word2}': {similarity:.2f}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy từ trong WordNet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Độ giống nhau giữa 'run' và 'walk': 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#10 Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai động từ đã cho.\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "word1 = \"run\"  # Nhập động từ 1\n",
    "word2 = \"walk\"  # Nhập động từ 2\n",
    "\n",
    "syn1 = wordnet.synsets(word1, pos=wordnet.VERB)\n",
    "syn2 = wordnet.synsets(word2, pos=wordnet.VERB)\n",
    "\n",
    "if syn1 and syn2:\n",
    "    similarity = syn1[0].wup_similarity(syn2[0])  # Tính độ giống nhau\n",
    "    print(f\"🔹 Độ giống nhau giữa '{word1}' và '{word2}': {similarity:.2f}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy từ trong WordNet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Số lượng tên nam: 2943\n",
      "🔸 Số lượng tên nữ: 5001\n",
      "\n",
      "📌 10 tên nam đầu tiên: ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim']\n",
      "📌 10 tên nữ đầu tiên: ['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\names.zip.\n"
     ]
    }
   ],
   "source": [
    "#11  Viết chương trình Python với thư viện NLTK để tìm số lượng tên nam và nữ trong các tên\n",
    "#kho ngữ liệu. In tên 10 nam và nữ đầu tiên. Lưu ý: Kho văn bản tên chứa tổng cộng khoảng\n",
    "#2943 nam (male.txt) và 5001 nữ (Female.txt) tên. Kho được biên soạn bởi Kantrowitz, Ross.\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "\n",
    "nltk.download(\"names\")\n",
    "\n",
    "male_names = names.words(\"male.txt\")\n",
    "female_names = names.words(\"female.txt\")\n",
    "\n",
    "print(f\"🔹 Số lượng tên nam: {len(male_names)}\")\n",
    "print(f\"🔸 Số lượng tên nữ: {len(female_names)}\")\n",
    "\n",
    "print(\"\\n📌 10 tên nam đầu tiên:\", male_names[:10])\n",
    "print(\"📌 10 tên nữ đầu tiên:\", female_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 15 tên ngẫu nhiên với nhãn giới tính:\n",
      "Zach → male\n",
      "Lari → female\n",
      "Ehud → male\n",
      "Hamil → male\n",
      "Adorne → female\n",
      "Bartolemo → male\n",
      "Zorana → female\n",
      "Joice → female\n",
      "Dane → male\n",
      "Paulo → male\n",
      "Catherin → female\n",
      "Nicolette → female\n",
      "Townsend → male\n",
      "Jens → male\n",
      "Manya → female\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#12.Viết chương trình Python với thư viện NLTK để in 15 kết hợp ngẫu nhiên đầu tiên được gắn\n",
    "#nhãn nam và được gắn nhãn tên nữ từ kho tên.\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "\n",
    "nltk.download(\"names\")\n",
    "\n",
    "male_names = [(name, \"male\") for name in names.words(\"male.txt\")]\n",
    "female_names = [(name, \"female\") for name in names.words(\"female.txt\")]\n",
    "\n",
    "# Gộp danh sách và xáo trộn ngẫu nhiên\n",
    "combined_names = male_names + female_names\n",
    "random.shuffle(combined_names)\n",
    "\n",
    "# In 15 cặp đầu tiên\n",
    "print(\"📌 15 tên ngẫu nhiên với nhãn giới tính:\")\n",
    "for name, gender in combined_names[:15]:\n",
    "    print(f\"{name} → {gender}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 15 ký tự cuối cùng của tên với nhãn:\n",
      "'r' → male\n",
      "'n' → male\n",
      "'y' → male\n",
      "'e' → male\n",
      "'t' → male\n",
      "'t' → male\n",
      "'y' → male\n",
      "'l' → male\n",
      "'l' → male\n",
      "'m' → male\n",
      "'h' → male\n",
      "'e' → male\n",
      "'l' → male\n",
      "'d' → male\n",
      "'r' → male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\TUF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#13 Viết chương trình Python với thư viện NLTK để trích xuất ký tự cuối cùng của tất cả các tên\n",
    "#được gắn nhãn và tạo mảng mới với chữ cái cuối cùng của mỗi tên và nhãn được liên kết.\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "\n",
    "nltk.download(\"names\")\n",
    "\n",
    "# Lấy danh sách tên nam và nữ cùng với nhãn\n",
    "male_names = [(name[-1], \"male\") for name in names.words(\"male.txt\")]\n",
    "female_names = [(name[-1], \"female\") for name in names.words(\"female.txt\")]\n",
    "\n",
    "# Gộp danh sách\n",
    "final_letters = male_names + female_names\n",
    "\n",
    "# In 15 ký tự cuối cùng đầu tiên cùng nhãn\n",
    "print(\"📌 15 ký tự cuối cùng của tên với nhãn:\")\n",
    "for letter, gender in final_letters[:15]:\n",
    "    print(f\"'{letter}' → {gender}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
